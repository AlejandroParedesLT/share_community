{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X80i_girFR2o"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bB8gHCR3FVC0"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCeYA79m1DEX"
      },
      "source": [
        "# Multi-task recommenders\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/multitask\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/multitask.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/multitask.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/multitask.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk8QEc4sIPMi"
      },
      "source": [
        "In the [basic retrieval tutorial](basic_retrieval) we built a retrieval system using movie watches as positive interaction signals.\n",
        "\n",
        "In many applications, however, there are multiple rich sources of feedback to draw upon. For example, an e-commerce site may record user visits to product pages (abundant, but relatively low signal), image clicks, adding to cart, and, finally, purchases. It may even record post-purchase signals such as reviews and returns.\n",
        "\n",
        "Integrating all these different forms of feedback is critical to building systems that users love to use, and that do not optimize for any one metric at the expense of overall performance.\n",
        "\n",
        "In addition, building a joint model for multiple tasks may produce better results than building a number of task-specific models. This is especially true where some data is abundant (for example, clicks), and some data is sparse (purchases, returns, manual reviews). In those scenarios, a joint model may be able to use representations learned from the abundant task to improve its predictions on the sparse task via a phenomenon known as [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). For example, [this paper](https://openreview.net/pdf?id=SJxPVcSonN) shows that a model predicting explicit user ratings from sparse user surveys can be substantially improved by adding an auxiliary task that uses abundant click log data.\n",
        "\n",
        "In this tutorial, we are going to build a multi-objective recommender for Movielens, using both implicit (movie watches) and explicit signals (ratings)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwrcZeK7x7xI"
      },
      "source": [
        "## Imports\n",
        "\n",
        "\n",
        "Let's first get our imports out of the way.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets\n",
        "\n",
        "!pip show tensorflow-datasets\n",
        "!pip show tensorflow-recommenders"
      ],
      "metadata": {
        "id": "-5vibiQfAxvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43d9663-494a-4cc3-d3d2-1434e9acbaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Name: tensorflow-datasets\n",
            "Version: 4.9.8\n",
            "Summary: tensorflow/datasets is a library of datasets ready to use with TensorFlow.\n",
            "Home-page: https://github.com/tensorflow/datasets\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, array_record, dm-tree, etils, immutabledict, numpy, promise, protobuf, psutil, pyarrow, requests, simple_parsing, tensorflow-metadata, termcolor, toml, tqdm, wrapt\n",
            "Required-by: \n",
            "Name: tensorflow-recommenders\n",
            "Version: 0.7.3\n",
            "Summary: Tensorflow Recommenders, a TensorFlow library for recommender systems.\n",
            "Home-page: https://github.com/tensorflow/recommenders\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: absl-py, tensorflow\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.25.2\n",
        "!pip install --upgrade pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_0_40rg4N3I",
        "outputId": "272f83c0-1263-4313-bd62-62068af7bceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZGYDaF-m5wZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PAqjR4a1RR4"
      },
      "source": [
        "## Preparing the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pref = pd.read_csv('/content/unified_preferences.csv')"
      ],
      "metadata": {
        "id": "Io3WJFUNvCSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref.dtypes"
      ],
      "metadata": {
        "id": "D3wTHu9Ab_oQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "032416f2-3c81-4dec-80ce-ab60fb445810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "user_id       int64\n",
              "item_id      object\n",
              "item_type     int64\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>user_id</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_id</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_type</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pref.head()"
      ],
      "metadata": {
        "id": "3ut2DiyvzMkq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c94f5f34-fd1f-4856-fcb0-e718251f9943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   user_id                 item_id  item_type\n",
              "0        1              0140042393          1\n",
              "1        1              0140121617          1\n",
              "2        1  4oU7NbUt2VfN8GBT4hnsrX          3\n",
              "3        1  3NLm801woJocONz1NmPJZR          3\n",
              "4        1  5FVd6KXrgO9B3JPmC8OPst          3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e7c9b5e-eef8-4358-8477-8a4c95b9f2e3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>item_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0140042393</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0140121617</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4oU7NbUt2VfN8GBT4hnsrX</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3NLm801woJocONz1NmPJZR</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5FVd6KXrgO9B3JPmC8OPst</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e7c9b5e-eef8-4358-8477-8a4c95b9f2e3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e7c9b5e-eef8-4358-8477-8a4c95b9f2e3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e7c9b5e-eef8-4358-8477-8a4c95b9f2e3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3f75343a-c3a3-4d81-91a8-8d75db7661c3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f75343a-c3a3-4d81-91a8-8d75db7661c3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3f75343a-c3a3-4d81-91a8-8d75db7661c3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pref"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pref_subset = pref[['item_id']].astype(str)\n",
        "pref['user_id'] = pref['user_id'].astype(str)"
      ],
      "metadata": {
        "id": "GXFo1uWIvbmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pref_subset = pref_subset['item_id'].unique()"
      ],
      "metadata": {
        "id": "7fsiLGMc1hmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_tf = tf.data.Dataset.from_tensor_slices({'item_id': pref_subset})\n",
        "watched_tf = tf.data.Dataset.from_tensor_slices(dict(pref))"
      ],
      "metadata": {
        "id": "dlswSZj8zKR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ySWtibjm_6a"
      },
      "outputs": [],
      "source": [
        "#ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "#movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
        "\n",
        "# Select the basic features.\n",
        "watched = watched_tf.map(lambda x: {\n",
        "    \"item_id\": x[\"item_id\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    #\"Book-Rating\": x[\"Book-Rating\"],\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items = items_tf.map(lambda x:\n",
        "    x[\"item_id\"]\n",
        "    #\"Book-Title\": x[\"Book-Title\"]\n",
        ")"
      ],
      "metadata": {
        "id": "eyliEd0owQTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRHorm8W1yf3"
      },
      "source": [
        "And repeat our preparations for building vocabularies and splitting the data into a train and a test set:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly shuffle data and split between train and test.\n",
        "tf.random.set_seed(42)\n",
        "shuffled = watched.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(5_000)\n",
        "test = shuffled.skip(5_000).take(2_000)\n",
        "\n",
        "item_titles = items.batch(1_000)\n",
        "user_ids = watched.batch(1_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_items = np.unique(np.concatenate(list(item_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ],
      "metadata": {
        "id": "T5VKTbSZ2iiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCi-seR86qqa"
      },
      "source": [
        "## A multi-task model\n",
        "\n",
        "There are two critical parts to multi-task recommenders:\n",
        "\n",
        "1. They optimize for two or more objectives, and so have two or more losses.\n",
        "2. They share variables between the tasks, allowing for transfer learning.\n",
        "\n",
        "In this tutorial, we will define our models as before, but instead of having  a single task, we will have two tasks: one that predicts ratings, and one that predicts movie watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHrk_SLzKCM"
      },
      "source": [
        "The user and movie models are as before:\n",
        "\n",
        "```python\n",
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add 1 to account for the unknown token.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])\n",
        "\n",
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWCwkE5z8QBe"
      },
      "source": [
        "However, now we will have two tasks. The first is the rating task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Ranking(\n",
        "    loss=tf.keras.losses.MeanSquaredError(),\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrgQIXEm8UWf"
      },
      "source": [
        "Its goal is to predict the ratings as accurately as possible.\n",
        "\n",
        "The second is the retrieval task:\n",
        "\n",
        "```python\n",
        "tfrs.tasks.Retrieval(\n",
        "    metrics=tfrs.metrics.FactorizedTopK(\n",
        "        candidates=movies.batch(128)\n",
        "    )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNrv7_gakmF"
      },
      "source": [
        "As before, this task's goal is to predict which movies the user will or will not watch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWw3xuq8mGh"
      },
      "source": [
        "### Putting it together\n",
        "\n",
        "We put it all together in a model class.\n",
        "\n",
        "The new component here is that - since we have two tasks and two losses - we need to decide on how important each loss is. We can do this by giving each of the losses a weight, and treating these weights as hyperparameters. If we assign a large loss weight to the rating task, our model is going to focus on predicting ratings (but still use some information from the retrieval task); if we assign a large loss weight to the retrieval task, it will focus on retrieval instead."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ItemModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, rating_weight: float, retrieval_weight: float, unique_user_ids, unique_items) -> None:\n",
        "    # We take the loss weights in the constructor: this allows us to instantiate\n",
        "    # several model objects with different loss weights.\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    embedding_dimension = 32\n",
        "    # Store values (optional, for rehydration or reference)\n",
        "    self.unique_user_ids = unique_user_ids\n",
        "    self.unique_items = unique_items\n",
        "\n",
        "\n",
        "    # User and item models.\n",
        "    self.item_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=list(self.unique_items), mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(self.unique_items) + 1, embedding_dimension)\n",
        "    ])\n",
        "    self.user_model: tf.keras.layers.Layer = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "        vocabulary=list(self.unique_user_ids), mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(self.unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    self.retrieval_task: tf.keras.layers.Layer = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=items.batch(128).map(self.item_model)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The loss weights.\n",
        "    self.retrieval_weight = retrieval_weight\n",
        "\n",
        "  def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the item features and pass them into the item model.\n",
        "    item_embeddings = self.item_model(features[\"item_id\"])\n",
        "\n",
        "    return (\n",
        "        user_embeddings,\n",
        "        item_embeddings\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "\n",
        "    #ratings = features.pop(\"Book-Rating\")\n",
        "\n",
        "    user_embeddings, item_embeddings = self(features)\n",
        "\n",
        "    # We compute the loss for each task.\n",
        "\n",
        "    retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)\n",
        "\n",
        "    # And combine them using the loss weights.\n",
        "    return (self.retrieval_weight * retrieval_loss)"
      ],
      "metadata": {
        "id": "YuIkBU8eeqpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngvn-c0b8lc2"
      },
      "source": [
        "### Rating-specialized model\n",
        "\n",
        "Depending on the weights we assign, the model will encode a different balance of the tasks. Let's start with a model that only considers ratings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lENViv04-i0T"
      },
      "source": [
        "The model does OK on predicting ratings (with an RMSE of around 1.11), but performs poorly at predicting which movies will be watched or not: its accuracy at 100 is almost 4 times worse than a model trained solely to predict watches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPYd9LtE-4Fm"
      },
      "source": [
        "### Retrieval-specialized model\n",
        "\n",
        "Let's now try a model that focuses on retrieval only."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "class ItemModel(tfrs.models.Model):\n",
        "\n",
        "    def __init__(self,\n",
        "                 unique_user_ids: list,\n",
        "                 unique_items: list,\n",
        "                 items=None,\n",
        "                 use_full_items=False,\n",
        "                 rating_weight: float = 0.0,\n",
        "                 retrieval_weight: float = 1.0) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        embedding_dimension = 32\n",
        "\n",
        "        # Convert lists to strings for serialization purposes\n",
        "        self.unique_user_ids = unique_user_ids\n",
        "        self.unique_items = unique_items\n",
        "        self.use_full_items = use_full_items\n",
        "        self.retrieval_weight = retrieval_weight\n",
        "        self.rating_weight = rating_weight\n",
        "\n",
        "        # We'll handle items differently - don't store it as instance attribute\n",
        "        # since it's not serializable\n",
        "\n",
        "        # User and item models\n",
        "        self.item_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_items, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_items) + 1, embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        self.user_model = tf.keras.Sequential([\n",
        "            tf.keras.layers.StringLookup(vocabulary=unique_user_ids, mask_token=None),\n",
        "            tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "        ])\n",
        "\n",
        "        # Initialize retrieval_task conditionally\n",
        "        if use_full_items and items is not None:\n",
        "            self.retrieval_task = tfrs.tasks.Retrieval(\n",
        "                metrics=tfrs.metrics.FactorizedTopK(\n",
        "                    candidates=items.batch(128).map(self.item_model)\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            # For inference only\n",
        "            self.retrieval_task = tfrs.tasks.Retrieval()\n",
        "\n",
        "    def get_config(self):\n",
        "        # Return a serialized configuration of the model\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"unique_user_ids\": self.unique_user_ids,\n",
        "            \"unique_items\": self.unique_items,\n",
        "            \"use_full_items\": self.use_full_items,\n",
        "            \"rating_weight\": self.rating_weight,\n",
        "            \"retrieval_weight\": self.retrieval_weight,\n",
        "            # Don't include items as it's not serializable\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def call(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features[\"user_id\"])\n",
        "        item_embeddings = self.item_model(features[\"item_id\"])\n",
        "        return user_embeddings, item_embeddings\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings, item_embeddings = self(features)\n",
        "        retrieval_loss = self.retrieval_task(user_embeddings, item_embeddings)\n",
        "        return self.retrieval_weight * retrieval_loss"
      ],
      "metadata": {
        "id": "WgUkzQdCBwLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfnkGd2G--Qt"
      },
      "outputs": [],
      "source": [
        "#model = ItemModel(rating_weight=0.0, retrieval_weight=1.0)\n",
        "\n",
        "# For training (full dataset)\n",
        "training_model = ItemModel(\n",
        "    unique_user_ids=unique_user_ids,\n",
        "    unique_items=unique_items,\n",
        "    items=items,  # full dataset\n",
        "    use_full_items=True\n",
        ")\n",
        "\n",
        "training_model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192)\n",
        "cached_test = test.batch(4096)"
      ],
      "metadata": {
        "id": "CYC-JrXM6cJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCCBdM7U_B11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4912778b-563f-40b3-c926-6de6db0b55ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1/1 [==============================] - 52s 52s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 2.0000e-04 - factorized_top_k/top_50_categorical_accuracy: 2.0000e-04 - factorized_top_k/top_100_categorical_accuracy: 0.0010 - loss: 42585.8359 - regularization_loss: 0.0000e+00 - total_loss: 42585.8359\n",
            "Epoch 2/3\n",
            "1/1 [==============================] - 35s 35s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0016 - factorized_top_k/top_10_categorical_accuracy: 0.0056 - factorized_top_k/top_50_categorical_accuracy: 0.0334 - factorized_top_k/top_100_categorical_accuracy: 0.0546 - loss: 42518.1602 - regularization_loss: 0.0000e+00 - total_loss: 42518.1602\n",
            "Epoch 3/3\n",
            "1/1 [==============================] - 36s 36s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0020 - factorized_top_k/top_10_categorical_accuracy: 0.0060 - factorized_top_k/top_50_categorical_accuracy: 0.0930 - factorized_top_k/top_100_categorical_accuracy: 0.2380 - loss: 42109.5039 - regularization_loss: 0.0000e+00 - total_loss: 42109.5039\n",
            "1/1 [==============================] - 15s 15s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0020 - factorized_top_k/top_100_categorical_accuracy: 0.0045 - loss: 15205.6562 - regularization_loss: 0.0000e+00 - total_loss: 15205.6562\n",
            "Retrieval top-100 accuracy: 0.004.\n"
          ]
        }
      ],
      "source": [
        "training_model.fit(cached_train, epochs=3)\n",
        "metrics = training_model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "print(f\"Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.\")\n",
        "#print(f\"Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trained_user_embeddings, trained_item_embeddings = training_model({\n",
        "      \"user_id\": np.array([\"1\"]),\n",
        "      \"item_id\": np.array([\"0140121617\"])\n",
        "  })\n",
        "print(trained_item_embeddings)\n",
        "print(trained_user_embeddings)"
      ],
      "metadata": {
        "id": "iAh_oOSVgDmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6e5c510-77a7-4e30-b6f5-0bf555001ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.04127008 -0.01936675 -0.03088157  0.03099892  0.0389069  -0.00787858\n",
            "   0.04839778 -0.00282758  0.00746353 -0.04187801  0.00544034  0.03695157\n",
            "  -0.02507916 -0.03115453  0.03315819  0.01706872  0.03784264  0.0069543\n",
            "   0.03963206 -0.01354958 -0.03667245  0.03697178  0.00860536  0.04972446\n",
            "  -0.00316328 -0.0353498  -0.02781876 -0.0162636  -0.00971051  0.04338316\n",
            "  -0.0157269   0.0432185 ]], shape=(1, 32), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.25009692 -0.0950899   0.3315301   0.05091683  0.28157902 -0.33136028\n",
            "  -0.03510268 -0.24232739 -0.19859926 -0.15339683 -0.03974396  0.19982424\n",
            "  -0.24403408 -0.07985645 -0.3015722  -0.2885814   0.13420612  0.31123894\n",
            "   0.2591144  -0.25782844 -0.05665149  0.32847017 -0.16935423 -0.25761044\n",
            "  -0.30116618  0.15880619 -0.26618752  0.235757    0.30834234  0.01258642\n",
            "  -0.16918182 -0.06821686]], shape=(1, 32), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Extract all the necessary components from the training model\n",
        "user_weights = training_model.user_model.get_weights()\n",
        "item_weights = training_model.item_model.get_weights()\n",
        "\n",
        "# 2. Save vocabularies separately\n",
        "import pickle\n",
        "with open('user_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(unique_user_ids, f)\n",
        "with open('item_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(unique_items, f)\n",
        "\n",
        "# 3. Save weights separately\n",
        "import numpy as np\n",
        "np.save('user_weights.npy', user_weights)\n",
        "np.save('item_weights.npy', item_weights)\n",
        "\n",
        "print(\"Model components saved successfully!\")\n",
        "\n",
        "\n",
        "def load_inference_model():\n",
        "    # Load vocabularies\n",
        "    with open('user_vocab.pkl', 'rb') as f:\n",
        "        user_vocab = pickle.load(f)\n",
        "    with open('item_vocab.pkl', 'rb') as f:\n",
        "        item_vocab = pickle.load(f)\n",
        "\n",
        "    # Load weights\n",
        "    user_weights = np.load('user_weights.npy', allow_pickle=True)\n",
        "    item_weights = np.load('item_weights.npy', allow_pickle=True)\n",
        "\n",
        "    # Create embedding lookup models\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    user_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(vocabulary=user_vocab, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(user_vocab) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    item_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(vocabulary=item_vocab, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(item_vocab) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Build models\n",
        "    user_model(np.array([user_vocab[0]]))\n",
        "    item_model(np.array([item_vocab[0]]))\n",
        "\n",
        "    # Set weights\n",
        "    user_model.set_weights(user_weights)\n",
        "    item_model.set_weights(item_weights)\n",
        "\n",
        "    # Create a simple inference model function\n",
        "    def get_embeddings(user_ids, item_ids):\n",
        "        user_embs = user_model(user_ids)\n",
        "        item_embs = item_model(item_ids)\n",
        "        return user_embs, item_embs\n",
        "\n",
        "    return get_embeddings, user_model, item_model\n",
        "\n",
        "# Example usage\n",
        "get_embeddings, user_model, item_model = load_inference_model()\n",
        "\n",
        "# Test it\n",
        "test_user_id = np.array([\"1\"])\n",
        "test_item_id = np.array([\"0140121617\"])\n",
        "user_emb, item_emb = get_embeddings(test_user_id, test_item_id)\n",
        "print(\"User embedding:\", user_emb)\n",
        "print(\"Item embedding:\", item_emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "G98J5aN_BajD",
        "outputId": "108596d7-b706-41fd-cf18-d0d3555c4742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type ndarray is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5b18092d4a90>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_config.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFwjUus_pLU"
      },
      "source": [
        "### Joint model\n",
        "\n",
        "Let's now train a model that assigns positive weights to both tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_rkOsaB3f9"
      },
      "source": [
        "The result is a model that performs roughly as well on both tasks as each specialized model.\n",
        "\n",
        "### Making prediction\n",
        "\n",
        "We can use the trained multitask model to get trained user and movie embeddings, as well as the predicted rating:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FADp0pUWINTD"
      },
      "source": [
        "While the results here do not show a clear accuracy benefit from a joint model in this case, multi-task learning is in general an extremely useful tool. We can expect better results when we can transfer knowledge from a data-abundant task (such as clicks) to a closely related data-sparse task (such as purchases)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import required libraries\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prepare user IDs and track IDs\n",
        "# Drop duplicate users to ensure one track per user\n",
        "user_rate_map = ratings_df.drop_duplicates(subset=[\"User-ID\"])\n",
        "all_user_ids = user_rate_map[\"User-ID\"].values\n",
        "all_book_ids = user_rate_map[\"ISBN\"].values\n",
        "\n",
        "# 2. Pass user IDs and track IDs into the model to obtain user embeddings\n",
        "track_embeddings, user_embeddings, rating_predictions = model({\n",
        "    \"User-ID\": all_user_ids,\n",
        "    \"ISBN\": all_book_ids\n",
        "})\n",
        "\n",
        "# 3. Convert user embeddings to a NumPy array\n",
        "user_embeddings_np = np.array(user_embeddings)\n",
        "\n",
        "# 4. Compute cosine similarity between all user embeddings\n",
        "similarity_matrix = cosine_similarity(user_embeddings_np)\n",
        "\n",
        "# 5. Create a DataFrame to map user IDs to each other based on similarity\n",
        "similarity_df = pd.DataFrame(similarity_matrix, index=all_user_ids, columns=all_user_ids)\n",
        "\n",
        "# 6. Find the top 5 most similar users to a specific user\n",
        "target_user_id = \"276726\"\n",
        "\n",
        "# Sort the similarities in descending order (highest similarity first)\n",
        "similar_users = similarity_df.loc[target_user_id].sort_values(ascending=False)\n",
        "\n",
        "# Exclude the user themselves (the first entry) and select the top 5\n",
        "top5_similar_users = similar_users[1:6]\n",
        "\n",
        "# 7. Print the result\n",
        "print(f\"\\n Top 5 users most similar to {target_user_id}:\")\n",
        "print(top5_similar_users)"
      ],
      "metadata": {
        "id": "gp2-dpyaCmI7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}